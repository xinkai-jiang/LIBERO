{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we walk through all the necessary components of running experiments on LIBERO, and some common usage such as defining your own algorithm and policy architectures in the codebase.\n",
    "\n",
    "1. Dataset preparation for your algorithms\n",
    "2. Write your own algorithm\n",
    "    - Subclassing from `Sequential` base class\n",
    "3. Write your own model\n",
    "4. Write your training loop\n",
    "5. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_904173/2563292850.py:18: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../libero/configs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'color_aug': { 'network': 'BatchWiseImgColorJitterAug',\n",
      "                 'network_kwargs': { 'brightness': 0.3,\n",
      "                                     'contrast': 0.3,\n",
      "                                     'epsilon': 0.1,\n",
      "                                     'hue': 0.3,\n",
      "                                     'input_shape': None,\n",
      "                                     'saturation': 0.3}},\n",
      "  'embed_size': 64,\n",
      "  'extra_hidden_size': 128,\n",
      "  'extra_num_layers': 0,\n",
      "  'image_encoder': { 'network': 'ResnetEncoder',\n",
      "                     'network_kwargs': { 'freeze': False,\n",
      "                                         'language_fusion': 'film',\n",
      "                                         'no_stride': False,\n",
      "                                         'pretrained': False,\n",
      "                                         'remove_layer_num': 4}},\n",
      "  'language_encoder': { 'network': 'MLPEncoder',\n",
      "                        'network_kwargs': { 'hidden_size': 128,\n",
      "                                            'input_size': 768,\n",
      "                                            'num_layers': 1,\n",
      "                                            'output_size': 128}},\n",
      "  'policy_head': { 'loss_kwargs': {'loss_coef': 1.0},\n",
      "                   'network': 'GMMHead',\n",
      "                   'network_kwargs': { 'activation': 'softplus',\n",
      "                                       'hidden_size': 1024,\n",
      "                                       'low_eval_noise': False,\n",
      "                                       'min_std': 0.0001,\n",
      "                                       'num_layers': 2,\n",
      "                                       'num_modes': 5}},\n",
      "  'policy_type': 'BCTransformerPolicy',\n",
      "  'temporal_position_encoding': { 'network': 'SinusoidalPositionEncoding',\n",
      "                                  'network_kwargs': { 'factor_ratio': None,\n",
      "                                                      'input_size': None,\n",
      "                                                      'inv_freq_factor': 10}},\n",
      "  'transformer_dropout': 0.1,\n",
      "  'transformer_head_output_size': 64,\n",
      "  'transformer_input_size': None,\n",
      "  'transformer_max_seq_len': 10,\n",
      "  'transformer_mlp_hidden_size': 256,\n",
      "  'transformer_num_heads': 6,\n",
      "  'transformer_num_layers': 4,\n",
      "  'translation_aug': { 'network': 'TranslationAug',\n",
      "                       'network_kwargs': { 'input_shape': None,\n",
      "                                           'translation': 8}}}\n",
      "('Note that the number of epochs used in this example is intentionally reduced '\n",
      " 'to 5.')\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: rgb with keys: ['agentview_rgb', 'eye_in_hand_rgb']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: low_dim with keys: ['gripper_states', 'joint_states']\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 172.53it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 201.64it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 119.13it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 181.62it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 209.67it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 185.39it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 221.76it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 180.52it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 185.06it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 189.48it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ead0fb888e4b2a8ebf86fec9eedf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a628110880f648e596bf154094e0e12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e775ff3196d4eb5922f8a4f2d44e37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f7891042564f1f932bf3bd0979eb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eb0367f7ec43bf8181f43e3070b0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "import hydra\n",
    "import pprint\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from libero.libero.benchmark import get_benchmark\n",
    "from libero.lifelong.datasets import (GroupedTaskDataset, SequenceVLDataset, get_dataset)\n",
    "from libero.lifelong.utils import (get_task_embs, safe_device, create_experiment_dir)\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "### load the default hydra config\n",
    "initialize(config_path=\"../libero/configs\")\n",
    "hydra_cfg = compose(config_name=\"config\")\n",
    "yaml_config = OmegaConf.to_yaml(hydra_cfg)\n",
    "cfg = EasyDict(yaml.safe_load(yaml_config))\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(cfg.policy)\n",
    "\n",
    "# prepare lifelong learning\n",
    "cfg.folder = get_libero_path(\"datasets\")\n",
    "cfg.bddl_folder = get_libero_path(\"bddl_files\")\n",
    "cfg.init_states_folder = get_libero_path(\"init_states\")\n",
    "cfg.eval.num_procs = 1\n",
    "cfg.eval.n_eval = 5\n",
    "\n",
    "cfg.train.n_epochs = 25\n",
    "\n",
    "pp.pprint(f\"Note that the number of epochs used in this example is intentionally reduced to 5.\")\n",
    "\n",
    "task_order = cfg.data.task_order_index # can be from {0 .. 21}, default to 0, which is [task 0, 1, 2 ...]\n",
    "cfg.benchmark_name = \"libero_object\" # can be from {\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_10\"}\n",
    "benchmark = get_benchmark(cfg.benchmark_name)(task_order)\n",
    "\n",
    "# prepare datasets from the benchmark\n",
    "datasets = []\n",
    "descriptions = []\n",
    "shape_meta = None\n",
    "n_tasks = benchmark.n_tasks\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    # currently we assume tasks from same benchmark have the same shape_meta\n",
    "    task_i_dataset, shape_meta = get_dataset(\n",
    "            dataset_path=os.path.join(cfg.folder, benchmark.get_task_demonstration(i)),\n",
    "            obs_modality=cfg.data.obs.modality,\n",
    "            initialize_obs_utils=(i==0),\n",
    "            seq_len=cfg.data.seq_len,\n",
    "    )\n",
    "    # add language to the vision dataset, hence we call vl_dataset\n",
    "    descriptions.append(benchmark.get_task(i).language)\n",
    "    datasets.append(task_i_dataset)\n",
    "\n",
    "task_embs = get_task_embs(cfg, descriptions)\n",
    "benchmark.set_task_embs(task_embs)\n",
    "\n",
    "datasets = [SequenceVLDataset(ds, emb) for (ds, emb) in zip(datasets, task_embs)]\n",
    "n_demos = [data.n_demos for data in datasets]\n",
    "n_sequences = [data.total_num_sequences for data in datasets]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write your own policy architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from libero.lifelong.models.modules.rgb_modules import *\n",
    "from libero.lifelong.models.modules.language_modules import *\n",
    "from libero.lifelong.models.base_policy import BasePolicy\n",
    "from libero.lifelong.models.policy_head import *\n",
    "from libero.lifelong.models.modules.transformer_modules import *\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# A model handling extra input modalities besides images at time t.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "class ExtraModalityTokens(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_joint=False,\n",
    "        use_gripper=False,\n",
    "        use_ee=False,\n",
    "        extra_num_layers=0,\n",
    "        extra_hidden_size=64,\n",
    "        extra_embedding_size=32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This is a class that maps all extra modality inputs into tokens of the same size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_joint = use_joint\n",
    "        self.use_gripper = use_gripper\n",
    "        self.use_ee = use_ee\n",
    "        self.extra_embedding_size = extra_embedding_size\n",
    "\n",
    "        joint_states_dim = 7\n",
    "        gripper_states_dim = 2\n",
    "        ee_dim = 3\n",
    "\n",
    "        self.num_extra = int(use_joint) + int(use_gripper) + int(use_ee)\n",
    "\n",
    "        extra_low_level_feature_dim = (\n",
    "            int(use_joint) * joint_states_dim\n",
    "            + int(use_gripper) * gripper_states_dim\n",
    "            + int(use_ee) * ee_dim\n",
    "        )\n",
    "\n",
    "        assert extra_low_level_feature_dim > 0, \"[error] no extra information\"\n",
    "\n",
    "        self.extra_encoders = {}\n",
    "\n",
    "        def generate_ptaskroprio_mlp_fn(modality_name, extra_low_level_feature_dim):\n",
    "            assert extra_low_level_feature_dim > 0  # we indeed have extra information\n",
    "            if extra_num_layers > 0:\n",
    "                layers = [nn.Linear(extra_low_level_feature_dim, extra_hidden_size)]\n",
    "                for i in range(1, extra_num_layers):\n",
    "                    layers += [\n",
    "                        nn.Linear(extra_hidden_size, extra_hidden_size),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                    ]\n",
    "                layers += [nn.Linear(extra_hidden_size, extra_embedding_size)]\n",
    "            else:\n",
    "                layers = [nn.Linear(extra_low_level_feature_dim, extra_embedding_size)]\n",
    "\n",
    "            self.proprio_mlp = nn.Sequential(*layers)\n",
    "            self.extra_encoders[modality_name] = {\"encoder\": self.proprio_mlp}\n",
    "\n",
    "        for (proprio_dim, use_modality, modality_name) in [\n",
    "            (joint_states_dim, self.use_joint, \"joint_states\"),\n",
    "            (gripper_states_dim, self.use_gripper, \"gripper_states\"),\n",
    "            (ee_dim, self.use_ee, \"ee_states\"),\n",
    "        ]:\n",
    "\n",
    "            if use_modality:\n",
    "                generate_proprio_mlp_fn(modality_name, proprio_dim)\n",
    "\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [x[\"encoder\"] for x in self.extra_encoders.values()]\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_dict):\n",
    "        \"\"\"\n",
    "        obs_dict: {\n",
    "            (optional) joint_stats: (B, T, 7),\n",
    "            (optional) gripper_states: (B, T, 2),\n",
    "            (optional) ee: (B, T, 3)\n",
    "        }\n",
    "        map above to a latent vector of shape (B, T, H)\n",
    "        \"\"\"\n",
    "        tensor_list = []\n",
    "\n",
    "        for (use_modality, modality_name) in [\n",
    "            (self.use_joint, \"joint_states\"),\n",
    "            (self.use_gripper, \"gripper_states\"),\n",
    "            (self.use_ee, \"ee_states\"),\n",
    "        ]:\n",
    "\n",
    "            if use_modality:\n",
    "                tensor_list.append(\n",
    "                    self.extra_encoders[modality_name][\"encoder\"](\n",
    "                        obs_dict[modality_name]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        x = torch.stack(tensor_list, dim=-2)\n",
    "        return x\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# A Transformer policy\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "class MyTransformerPolicy(BasePolicy):\n",
    "    \"\"\"\n",
    "    Input: (o_{t-H}, ... , o_t)\n",
    "    Output: a_t or distribution of a_t\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, shape_meta):\n",
    "        super().__init__(cfg, shape_meta)\n",
    "        policy_cfg = cfg.policy\n",
    "\n",
    "        ### 1. encode image\n",
    "        embed_size = policy_cfg.embed_size\n",
    "        transformer_input_sizes = []\n",
    "        self.image_encoders = {}\n",
    "        for name in shape_meta[\"all_shapes\"].keys():\n",
    "            if \"rgb\" in name or \"depth\" in name:\n",
    "                kwargs = policy_cfg.image_encoder.network_kwargs\n",
    "                kwargs.input_shape = shape_meta[\"all_shapes\"][name]\n",
    "                kwargs.output_size = embed_size\n",
    "                kwargs.language_dim = (\n",
    "                    policy_cfg.language_encoder.network_kwargs.input_size\n",
    "                )\n",
    "                self.image_encoders[name] = {\n",
    "                    \"input_shape\": shape_meta[\"all_shapes\"][name],\n",
    "                    \"encoder\": eval(policy_cfg.image_encoder.network)(**kwargs),\n",
    "                }\n",
    "\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [x[\"encoder\"] for x in self.image_encoders.values()]\n",
    "        )\n",
    "\n",
    "        ### 2. encode language\n",
    "        policy_cfg.language_encoder.network_kwargs.output_size = embed_size\n",
    "        self.language_encoder = eval(policy_cfg.language_encoder.network)(\n",
    "            **policy_cfg.language_encoder.network_kwargs\n",
    "        )\n",
    "\n",
    "        ### 3. encode extra information (e.g. gripper, joint_state)\n",
    "        self.extra_encoder = ExtraModalityTokens(\n",
    "            use_joint=cfg.data.use_joint,\n",
    "            use_gripper=cfg.data.use_gripper,\n",
    "            use_ee=cfg.data.use_ee,\n",
    "            extra_num_layers=policy_cfg.extra_num_layers,\n",
    "            extra_hidden_size=policy_cfg.extra_hidden_size,\n",
    "            extra_embedding_size=embed_size,\n",
    "        )\n",
    "\n",
    "        ### 4. define temporal transformer\n",
    "        policy_cfg.temporal_position_encoding.network_kwargs.input_size = embed_size\n",
    "        self.temporal_position_encoding_fn = eval(\n",
    "            policy_cfg.temporal_position_encoding.network\n",
    "        )(**policy_cfg.temporal_position_encoding.network_kwargs)\n",
    "\n",
    "        self.temporal_transformer = TransformerDecoder(\n",
    "            input_size=embed_size,\n",
    "            num_layers=policy_cfg.transformer_num_layers,\n",
    "            num_heads=policy_cfg.transformer_num_heads,\n",
    "            head_output_size=policy_cfg.transformer_head_output_size,\n",
    "            mlp_hidden_size=policy_cfg.transformer_mlp_hidden_size,\n",
    "            dropout=policy_cfg.transformer_dropout,\n",
    "        )\n",
    "\n",
    "        policy_head_kwargs = policy_cfg.policy_head.network_kwargs\n",
    "        policy_head_kwargs.input_size = embed_size\n",
    "        policy_head_kwargs.output_size = shape_meta[\"ac_dim\"]\n",
    "\n",
    "        self.policy_head = eval(policy_cfg.policy_head.network)(\n",
    "            **policy_cfg.policy_head.loss_kwargs,\n",
    "            **policy_cfg.policy_head.network_kwargs\n",
    "        )\n",
    "\n",
    "        self.latent_queue = []\n",
    "        self.max_seq_len = policy_cfg.transformer_max_seq_len\n",
    "\n",
    "    def temporal_encode(self, x):\n",
    "        pos_emb = self.temporal_position_encoding_fn(x)\n",
    "        x = x + pos_emb.unsqueeze(1)  # (B, T, num_modality, E)\n",
    "        sh = x.shape\n",
    "        self.temporal_transformer.compute_mask(x.shape)\n",
    "\n",
    "        x = TensorUtils.join_dimensions(x, 1, 2)  # (B, T*num_modality, E)\n",
    "        x = self.temporal_transformer(x)\n",
    "        x = x.reshape(*sh)\n",
    "        return x[:, :, 0]  # (B, T, E)\n",
    "\n",
    "    def spatial_encode(self, data):\n",
    "        # 1. encode extra\n",
    "        extra = self.extra_encoder(data[\"obs\"])  # (B, T, num_extra, E)\n",
    "\n",
    "        # 2. encode language, treat it as action token\n",
    "        B, T = extra.shape[:2]\n",
    "        text_encoded = self.language_encoder(data)  # (B, E)\n",
    "        text_encoded = text_encoded.view(B, 1, 1, -1).expand(\n",
    "            -1, T, -1, -1\n",
    "        )  # (B, T, 1, E)\n",
    "        encoded = [text_encoded, extra]\n",
    "\n",
    "        # 3. encode image\n",
    "        for img_name in self.image_encoders.keys():\n",
    "            x = data[\"obs\"][img_name]\n",
    "            B, T, C, H, W = x.shape\n",
    "            img_encoded = self.image_encoders[img_name][\"encoder\"](\n",
    "                x.reshape(B * T, C, H, W),\n",
    "                langs=data[\"task_emb\"]\n",
    "                .reshape(B, 1, -1)\n",
    "                .repeat(1, T, 1)\n",
    "                .reshape(B * T, -1),\n",
    "            ).view(B, T, 1, -1)\n",
    "            encoded.append(img_encoded)\n",
    "        encoded = torch.cat(encoded, -2)  # (B, T, num_modalities, E)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.spatial_encode(data)\n",
    "        x = self.temporal_encode(x)\n",
    "        dist = self.policy_head(x)\n",
    "        return dist\n",
    "\n",
    "    def get_action(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            data = self.preprocess_input(data, train_mode=False)\n",
    "            x = self.spatial_encode(data)\n",
    "            self.latent_queue.append(x)\n",
    "            if len(self.latent_queue) > self.max_seq_len:\n",
    "                self.latent_queue.pop(0)\n",
    "            x = torch.cat(self.latent_queue, dim=1)  # (B, T, H_all)\n",
    "            x = self.temporal_encode(x)\n",
    "            dist = self.policy_head(x[:, -1])\n",
    "        action = dist.sample().detach().cpu()\n",
    "        return action.view(action.shape[0], -1).numpy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.latent_queue = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write your own lifelong learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libero.lifelong.algos.base import Sequential\n",
    "\n",
    "### All lifelong learning algorithm should inherit the Sequential algorithm super class\n",
    "\n",
    "class MyLifelongAlgo(Sequential):\n",
    "    \"\"\"\n",
    "    The experience replay policy.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_tasks,\n",
    "                 cfg,\n",
    "                 **policy_kwargs):\n",
    "        super().__init__(n_tasks=n_tasks, cfg=cfg, **policy_kwargs)\n",
    "        # define the learning policy\n",
    "        self.datasets = []\n",
    "        self.policy = eval(cfg.policy.policy_type)(cfg, cfg.shape_meta)\n",
    "\n",
    "    def start_task(self, task):\n",
    "        # what to do at the beginning of a new task\n",
    "        super().start_task(task)\n",
    "\n",
    "    def end_task(self, dataset, task_id, benchmark):\n",
    "        # what to do when finish learning a new task\n",
    "        self.datasets.append(dataset)\n",
    "\n",
    "    def observe(self, data):\n",
    "        # how the algorithm observes a data and returns a loss to be optimized\n",
    "        loss = super().observe(data)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Write your training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment directory is:  ./experiments/libero_object/MyLifelongAlgo/MyTransformerPolicy_seed10000/run_002\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_proprio_mlp_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibero\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlifelong\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_loss, evaluate_success\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment directory is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mexperiment_dir)\n\u001b[0;32m---> 12\u001b[0m algo \u001b[38;5;241m=\u001b[39m safe_device(\u001b[43mMyLifelongAlgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m, cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m result_summary \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_conf_mat\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mzeros((n_tasks, n_tasks)),   \u001b[38;5;66;03m# loss confusion matrix\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS_conf_mat\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mzeros((n_tasks, n_tasks)),   \u001b[38;5;66;03m# success confusion matrix\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_fwd\u001b[39m\u001b[38;5;124m'\u001b[39m     : np\u001b[38;5;241m.\u001b[39mzeros((n_tasks,)),           \u001b[38;5;66;03m# loss AUC, how fast the agent learns\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS_fwd\u001b[39m\u001b[38;5;124m'\u001b[39m     : np\u001b[38;5;241m.\u001b[39mzeros((n_tasks,)),           \u001b[38;5;66;03m# success AUC, how fast the agent succeeds\u001b[39;00m\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m gsz \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtask_group_size\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mMyLifelongAlgo.__init__\u001b[0;34m(self, n_tasks, cfg, **policy_kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     10\u001b[0m              n_tasks,\n\u001b[1;32m     11\u001b[0m              cfg,\n\u001b[1;32m     12\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpolicy_kwargs):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# define the learning policy\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/repository/LIBERO/libero/lifelong/algos/base.py:73\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, n_tasks, cfg)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_dir \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mexperiment_dir\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mlifelong\u001b[38;5;241m.\u001b[39malgo\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[43mget_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 154\u001b[0m, in \u001b[0;36mMyTransformerPolicy.__init__\u001b[0;34m(self, cfg, shape_meta)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(policy_cfg\u001b[38;5;241m.\u001b[39mlanguage_encoder\u001b[38;5;241m.\u001b[39mnetwork)(\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpolicy_cfg\u001b[38;5;241m.\u001b[39mlanguage_encoder\u001b[38;5;241m.\u001b[39mnetwork_kwargs\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m### 3. encode extra information (e.g. gripper, joint_state)\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mExtraModalityTokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_joint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_joint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gripper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_gripper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_ee\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_ee\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_num_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_num_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_hidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_hidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_embedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m### 4. define temporal transformer\u001b[39;00m\n\u001b[1;32m    164\u001b[0m policy_cfg\u001b[38;5;241m.\u001b[39mtemporal_position_encoding\u001b[38;5;241m.\u001b[39mnetwork_kwargs\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m=\u001b[39m embed_size\n",
      "Cell \u001b[0;32mIn[4], line 76\u001b[0m, in \u001b[0;36mExtraModalityTokens.__init__\u001b[0;34m(self, use_joint, use_gripper, use_ee, extra_num_layers, extra_hidden_size, extra_embedding_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (proprio_dim, use_modality, modality_name) \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     70\u001b[0m     (joint_states_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_joint, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoint_states\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     71\u001b[0m     (gripper_states_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gripper, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgripper_states\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     72\u001b[0m     (ee_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ee, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mee_states\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     73\u001b[0m ]:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_modality:\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mgenerate_proprio_mlp_fn\u001b[49m(modality_name, proprio_dim)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m     79\u001b[0m     [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_encoders\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     80\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_proprio_mlp_fn' is not defined"
     ]
    }
   ],
   "source": [
    "cfg.policy.policy_type = \"MyTransformerPolicy\"\n",
    "cfg.lifelong.algo = \"MyLifelongAlgo\"\n",
    "\n",
    "create_experiment_dir(cfg)\n",
    "cfg.shape_meta = shape_meta\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from libero.lifelong.metric import evaluate_loss, evaluate_success\n",
    "\n",
    "print(\"experiment directory is: \", cfg.experiment_dir)\n",
    "algo = safe_device(MyLifelongAlgo(n_tasks, cfg), cfg.device)\n",
    "\n",
    "result_summary = {\n",
    "    'L_conf_mat': np.zeros((n_tasks, n_tasks)),   # loss confusion matrix\n",
    "    'S_conf_mat': np.zeros((n_tasks, n_tasks)),   # success confusion matrix\n",
    "    'L_fwd'     : np.zeros((n_tasks,)),           # loss AUC, how fast the agent learns\n",
    "    'S_fwd'     : np.zeros((n_tasks,)),           # success AUC, how fast the agent succeeds\n",
    "}\n",
    "\n",
    "gsz = cfg.data.task_group_size\n",
    "\n",
    "if (cfg.train.n_epochs < 50):\n",
    "    print(\"NOTE: the number of epochs used in this example is intentionally reduced to 30 for simplicity.\")\n",
    "if (cfg.eval.n_eval < 20):\n",
    "    print(\"NOTE: the number of evaluation episodes used in this example is intentionally reduced to 5 for simplicity.\")\n",
    "\n",
    "for i in trange(n_tasks):\n",
    "    algo.train()\n",
    "    s_fwd, l_fwd = algo.learn_one_task(datasets[i], i, benchmark, result_summary)\n",
    "    # s_fwd is success rate AUC, when the agent learns the {0, e, 2e, ...} epochs\n",
    "    # l_fwd is BC loss AUC, similar to s_fwd\n",
    "    result_summary[\"S_fwd\"][i] = s_fwd\n",
    "    result_summary[\"L_fwd\"][i] = l_fwd\n",
    "\n",
    "    if cfg.eval.eval:\n",
    "        algo.eval()\n",
    "        # we only evaluate on the past tasks: 0 .. i\n",
    "        L = evaluate_loss(cfg, algo, benchmark, datasets[:i+1]) # (i+1,)\n",
    "        S = evaluate_success(cfg, algo, benchmark, list(range((i+1)*gsz))) # (i+1,)\n",
    "        result_summary[\"L_conf_mat\"][i][:i+1] = L\n",
    "        result_summary[\"S_conf_mat\"][i][:i+1] = S\n",
    "\n",
    "        torch.save(result_summary, os.path.join(cfg.experiment_dir, f'result.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './experiments/libero_object/MyLifelongAlgo/MyTransformerPolicy_seed10000/run_002/result.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_summary \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_conf_mat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_fwd\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/libero/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/libero/lib/python3.8/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/libero/lib/python3.8/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './experiments/libero_object/MyLifelongAlgo/MyTransformerPolicy_seed10000/run_002/result.pt'"
     ]
    }
   ],
   "source": [
    "result_summary = torch.load(os.path.join(cfg.experiment_dir, f'result.pt'))\n",
    "print(result_summary[\"S_conf_mat\"])\n",
    "print(result_summary[\"S_fwd\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compute FWT, BWT, and AUC of the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "benchmark_map = {\n",
    "    \"libero_10\"     : \"LIBERO_10\",\n",
    "    \"libero_90\"     : \"LIBERO_90\",\n",
    "    \"libero_spatial\": \"LIBERO_SPATIAL\",\n",
    "    \"libero_object\" : \"LIBERO_OBJECT\",\n",
    "    \"libero_goal\"   : \"LIBERO_GOAL\",\n",
    "}\n",
    "\n",
    "algo_map = {\n",
    "    \"base\"     : \"Sequential\",\n",
    "    \"er\"       : \"ER\",\n",
    "    \"ewc\"      : \"EWC\",\n",
    "    \"packnet\"  : \"PackNet\",\n",
    "    \"multitask\": \"Multitask\",\n",
    "    \"custom_algo\"   : \"MyLifelongAlgo\",\n",
    "}\n",
    "\n",
    "policy_map = {\n",
    "    \"bc_rnn_policy\"        : \"BCRNNPolicy\",\n",
    "    \"bc_transformer_policy\": \"BCTransformerPolicy\",\n",
    "    \"bc_vilt_policy\"       : \"BCViLTPolicy\",\n",
    "    \"custom_policy\"        : \"MyTransformerPolicy\",\n",
    "}\n",
    "\n",
    "seeds = [10000]\n",
    "N_SEEDS = len(seeds)\n",
    "N_TASKS = 10\n",
    "\n",
    "def get_auc(experiment_dir, bench, algo, policy):\n",
    "    N_EP = cfg.train.n_epochs // cfg.eval.eval_every + 1\n",
    "    fwds = np.zeros((N_TASKS, N_EP, N_SEEDS))\n",
    "\n",
    "    for task in range(N_TASKS):\n",
    "        counter = 0\n",
    "        for k, seed in enumerate(seeds):\n",
    "            name = f\"{experiment_dir}/task{task}_auc.log\"\n",
    "            try:\n",
    "                succ = torch.load(name)[\"success\"] # (n_epochs)\n",
    "                idx = succ.argmax()\n",
    "                succ[idx:] = succ[idx]\n",
    "                fwds[task, :, k] = succ\n",
    "            except:\n",
    "                print(\"Some errors when loading results\")\n",
    "                continue\n",
    "    return fwds\n",
    "\n",
    "def compute_metric(res):\n",
    "    mat, fwts  = res # fwds: (num_tasks, num_save_intervals, num_seeds)\n",
    "    num_tasks, num_seeds = mat.shape[1:]\n",
    "    ret = {}\n",
    "\n",
    "    # compute fwt\n",
    "    fwt = fwts.mean(axis=(0,1))\n",
    "    ret[\"fwt\"] = fwt\n",
    "    # compute bwt\n",
    "    bwts = []\n",
    "    aucs = []\n",
    "    for seed in range(num_seeds):\n",
    "        bwt = 0.0\n",
    "        auc = 0.0\n",
    "        for k in range(num_tasks):\n",
    "            bwt_k = 0.0\n",
    "            auc_k = 0.0\n",
    "            for tau in range(k+1, num_tasks):\n",
    "                bwt_k += mat[k,k,seed] - mat[tau,k,seed]\n",
    "                auc_k += mat[tau,k,seed]\n",
    "            if k + 1 < num_tasks:\n",
    "                bwt_k /= (num_tasks - k - 1)\n",
    "            auc_k = (auc_k + fwts[k,:,seed].mean()) / (num_tasks - k)\n",
    "\n",
    "            bwt += bwt_k\n",
    "            auc += auc_k\n",
    "        bwts.append(bwt / num_tasks)\n",
    "        aucs.append(auc / num_tasks)\n",
    "    bwts = np.array(bwts)\n",
    "    aucs = np.array(aucs)\n",
    "    ret[\"bwt\"] = bwts\n",
    "    ret[\"auc\"] = aucs\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fwt': array([0.60333333]), 'bwt': array([0.54126984]), 'auc': array([0.26577513])}\n"
     ]
    }
   ],
   "source": [
    "experiment_dir = \"experiments\"\n",
    "benchmark_name = \"libero_object\"\n",
    "algo_name = \"custom_algo\"\n",
    "policy_name = \"custom_policy\"\n",
    "\n",
    "fwds = get_auc(cfg.experiment_dir, benchmark_name, algo_name, policy_name)\n",
    "\n",
    "conf_mat = result_summary[\"S_conf_mat\"][..., np.newaxis]\n",
    "\n",
    "metric = compute_metric((conf_mat, fwds))\n",
    "print(metric)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize policy rollouts\n",
    "\n",
    " This is an example of how to use the trained model to do inference. We will take the policy from training on the first task as an example. More concrete example, please see `evaluate_one_task_success` in the file `lifelong/lifelong/metric.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/xinkai/miniconda3/envs/libero/lib/python3.8/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n",
      "/home/xinkai/miniconda3/envs/libero/lib/python3.8/site-packages/thop/profile.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) < LooseVersion(\"1.0.0\"):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'benchmark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# If it's packnet, the weights need to be processed first\u001b[39;00m\n\u001b[1;32m     14\u001b[0m task_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[0;32m---> 15\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241m.\u001b[39mget_task(task_id)\n\u001b[1;32m     16\u001b[0m task_emb \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mget_task_emb(task_id)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mlifelong\u001b[38;5;241m.\u001b[39malgo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackNet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'benchmark' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import imageio\n",
    "\n",
    "from libero.libero.envs import OffScreenRenderEnv, DummyVectorEnv\n",
    "from libero.lifelong.metric import raw_obs_to_tensor_obs\n",
    "\n",
    "# You can turn on subprocess\n",
    "env_num = 1\n",
    "action_dim = 7\n",
    "\n",
    "\n",
    "# If it's packnet, the weights need to be processed first\n",
    "task_id = 9\n",
    "task = benchmark.get_task(task_id)\n",
    "task_emb = benchmark.get_task_emb(task_id)\n",
    "\n",
    "if cfg.lifelong.algo == \"PackNet\":\n",
    "    algo = algo.get_eval_algo(task_id)\n",
    "\n",
    "algo.eval()\n",
    "env_args = {\n",
    "    \"bddl_file_name\": os.path.join(\n",
    "        cfg.bddl_folder, task.problem_folder, task.bddl_file\n",
    "    ),\n",
    "    \"camera_heights\": cfg.data.img_h,\n",
    "    \"camera_widths\": cfg.data.img_w,\n",
    "}\n",
    "\n",
    "env = DummyVectorEnv(\n",
    "            [lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)]\n",
    ")\n",
    "\n",
    "init_states_path = os.path.join(\n",
    "    cfg.init_states_folder, task.problem_folder, task.init_states_file\n",
    ")\n",
    "init_states = torch.load(init_states_path)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "init_state = init_states[0:1]\n",
    "dones = [False]\n",
    "\n",
    "algo.reset()\n",
    "\n",
    "obs = env.set_init_state(init_state)\n",
    "\n",
    "\n",
    "# Make sure the gripepr is open to make it consistent with the provided demos.\n",
    "dummy_actions = np.zeros((env_num, action_dim))\n",
    "for _ in range(5):\n",
    "    obs, _, _, _ = env.step(dummy_actions)\n",
    "\n",
    "steps = 0\n",
    "\n",
    "obs_tensors = [[]] * env_num\n",
    "while steps < cfg.eval.max_steps:\n",
    "    steps += 1\n",
    "    data = raw_obs_to_tensor_obs(obs, task_emb, cfg)\n",
    "    action = algo.policy.get_action(data)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    for k in range(env_num):\n",
    "        dones[k] = dones[k] or done[k]\n",
    "        obs_tensors[k].append(obs[k][\"agentview_image\"])\n",
    "    if all(dones):\n",
    "        break\n",
    "\n",
    "# visualize video\n",
    "# obs_tensor: (env_num, T, H, W, C)\n",
    "\n",
    "images = [img[::-1] for img in obs_tensors[0]]\n",
    "fps = 30\n",
    "writer  = imageio.get_writer('tmp_video.mp4', fps=fps)\n",
    "for image in images:\n",
    "    writer.append_data(image)\n",
    "writer.close()\n",
    "\n",
    "video_data = open(\"tmp_video.mp4\", \"rb\").read()\n",
    "video_tag = f'<video controls alt=\"test\" src=\"data:video/mp4;base64,{b64encode(video_data).decode()}\">'\n",
    "HTML(data=video_tag)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
